{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Particle Filtering in Gen\n",
    "\n",
    "Sequential Monte Carlo (SMC) methods such as particle filtering iteratively solve a *sequence of inference problems* using techniques based on importance sampling and in some cases MCMC. The solution to each problem in the sequence is represented as a collection of samples or *particles*. The particles for each problem are based on extending or adjusting the particles for the previous problem in the sequence.\n",
    "\n",
    "The sequence of inference problems that are solved often arise naturally from observations that arrive incrementally, as in particle filtering. The problems can also be constructed instrumentally to facilitate inference, as in annealed importance sampling [3]. This tutorial shows how to use Gen to implement a particle filter for a tracking problem that uses \"rejuvenation\" MCMC moves. Specifically, we will address the \"bearings only tracking\" problem described in [4].\n",
    "\n",
    "[1] Doucet, Arnaud, Nando De Freitas, and Neil Gordon. \"An introduction to sequential Monte Carlo methods.\" Sequential Monte Carlo methods in practice. Springer, New York, NY, 2001. 3-14.\n",
    "\n",
    "[2] Del Moral, Pierre, Arnaud Doucet, and Ajay Jasra. \"Sequential monte carlo samplers.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68.3 (2006): 411-436.\n",
    "\n",
    "[3] Neal, Radford M. \"Annealed importance sampling.\" Statistics and computing 11.2 (2001): 125-139.\n",
    "\n",
    "[4] Gilks, Walter R., and Carlo Berzuini. \"Following a moving targetâ€”Monte Carlo inference for dynamic Bayesian models.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 63.1 (2001): 127-146. [PDF](http://www.mathcs.emory.edu/~whalen/Papers/BNs/MonteCarlo-DBNs.pdf)\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Section 1**: [Implementing the generative model](#basic-model)\n",
    "\n",
    "**Section 2**: [Implementing a basic particle filter](#basic-pf)\n",
    "\n",
    "**Section 3**: [Adding rejuvenation moves](#rejuv)\n",
    "\n",
    "**Section 4**: [Using the unfold combinator to improve performance](#unfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Gen\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing the generative model <a name=\"basic-model\"></a>\n",
    "\n",
    "We will implement a generative model for the movement of a point in the x-y plane and bearing measurements of the location of this point relative to the origin over time.\n",
    "\n",
    "We assume that we know the approximate initial position and velocity of the point. We assume the point's x- and y- velocity are subject to random perturbations drawn from some normal distribution with a known variance. Each bearing measurement consists of the angle of the point being tracked relative to the positive x-axis.\n",
    "\n",
    "We write the generative model as a generative function below. The function first samples the initial state of the point from a prior distribution, and then generates `T` successive states in a `for` loop. The argument to the model is the number of states not including the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bearing(x, y) = atan(y, x)\n",
    "\n",
    "@gen function model(T::Int)\n",
    "    \n",
    "    measurement_noise = 0.005\n",
    "    velocity_var = (1.0/1e6)\n",
    "\n",
    "    xs = Vector{Float64}(undef, T+1)\n",
    "    ys = Vector{Float64}(undef, T+1)\n",
    "\n",
    "    # prior on initial x-coordinate\n",
    "    x = @trace(normal(0.01, 0.01), :x0)\n",
    "       \n",
    "    # prior on initial y-coordinate\n",
    "    y = @trace(normal(0.95, 0.01), :y0)\n",
    "    \n",
    "    # prior on x-component of initial velocity\n",
    "    vx = @trace(normal(0.002, 0.01), :vx0)\n",
    "    \n",
    "    # prior on y-component of initial velocity\n",
    "    vy = @trace(normal(-0.013, 0.01), :vy0)\n",
    "    \n",
    "    # initial bearing measurement\n",
    "    @trace(normal(bearing(x, y), measurement_noise), :z0)\n",
    "\n",
    "    # record position\n",
    "    xs[1] = x\n",
    "    ys[1] = y\n",
    "    \n",
    "    # generate successive states and measurements\n",
    "    for t=1:T\n",
    "        \n",
    "        # update the state of the point\n",
    "        vx = @trace(normal(vx, sqrt(velocity_var)), (:vx, t))\n",
    "        vy = @trace(normal(vy, sqrt(velocity_var)), (:vy, t))\n",
    "        x += vx\n",
    "        y += vy\n",
    "        \n",
    "        # bearing measurement\n",
    "        @trace(normal(bearing(x, y), measurement_noise), (:z, t))\n",
    "\n",
    "        # record position\n",
    "        xs[t+1] = x\n",
    "        ys[t+1] = y\n",
    "    end\n",
    "    \n",
    "    # return the sequence of positions\n",
    "    return (xs, ys)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a data set of positions, and observed bearings, by sampling from this model, with `T=50`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Random\n",
    "Random.seed!(4)\n",
    "\n",
    "# generate trace with specific initial conditions\n",
    "T = 50\n",
    "constraints = Gen.choicemap((:x0, 0.01), (:y0, 0.95), (:vx0, 0.002), (:vy0, -0.013))\n",
    "(trace, _) = Gen.generate(model, (T,), constraints)\n",
    "\n",
    "# extract the observed data (zs) from the trace\n",
    "choices = Gen.get_choices(trace)\n",
    "zs = Vector{Float64}(undef, T+1)\n",
    "zs[1] = choices[:z0]\n",
    "for t=1:T\n",
    "    zs[t+1] = choices[(:z, t)]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next write a visualization for traces of this model below. It shows the positions and dots and the observed bearings as lines from the origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function render(trace; show_data=true, max_T=get_args(trace)[1])\n",
    "    (T,) = Gen.get_args(trace)\n",
    "    choices = Gen.get_choices(trace)\n",
    "    (xs, ys) = Gen.get_retval(trace)\n",
    "    zs = Vector{Float64}(undef, T+1)\n",
    "    zs[1] = choices[:z0]\n",
    "    for t=1:T\n",
    "        zs[t+1] = choices[(:z, t)]\n",
    "    end\n",
    "    scatter(xs[1:max_T+1], ys[1:max_T+1], s=5)\n",
    "    if show_data\n",
    "        for z in zs[1:max_T+1]\n",
    "            dx = cos(z) * 0.5\n",
    "            dy = sin(z) * 0.5\n",
    "            plot([0., dx], [0., dy], color=\"red\", alpha=0.3)\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the synthetic trace below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing a basic particle filter <a name=\"basic-pf\"></a>\n",
    "\n",
    "In Gen, a **particle is represented as a trace** and the particle filter state contains a weighted collection of traces. Below we define an inference program that runs a particle filter on an observed data set of bearings (`zs`). We use `num_particles` particles internally, and then we return a sample of `num_samples` traces from the weighted collection that the particle filter produces.\n",
    "\n",
    "Gen provides methods for initializing and updating the state of a particle filter, documented in [Particle Filtering](https://probcomp.github.io/Gen/dev/ref/inference/#Particle-Filtering-1).\n",
    "\n",
    "- `Gen.initialize_particle_filter`\n",
    "\n",
    "- `Gen.particle_filter_step!`\n",
    "\n",
    "Both of these methods can used either with the default proposal or a custom proposal. In this tutorial, we will use the default proposal. There is also a method that resamples particles based on their weights, which serves to redistribute the particles to more promising parts of the latent space.\n",
    "\n",
    "- `Gen.maybe_resample!`\n",
    "\n",
    "Gen also provides a method for sampling a collection of unweighted traces from the current weighted collection in the particle filter state:\n",
    "\n",
    "- `Gen.sample_unweighted_traces`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function particle_filter(num_particles::Int, zs::Vector{Float64}, num_samples::Int)\n",
    "    \n",
    "    # construct initial observations\n",
    "    init_obs = Gen.choicemap((:z0, zs[1]))\n",
    "    state = Gen.initialize_particle_filter(model, (0,), init_obs, num_particles)\n",
    "    \n",
    "    # steps\n",
    "    for t=1:length(zs)-1\n",
    "        Gen.maybe_resample!(state, ess_threshold=num_particles/2)\n",
    "        obs = Gen.choicemap(((:z, t), zs[t+1]))\n",
    "        Gen.particle_filter_step!(state, (t,), (UnknownChange(),), obs)\n",
    "    end\n",
    "    \n",
    "    # return a sample of unweighted traces from the weighted collection\n",
    "    return Gen.sample_unweighted_traces(state, num_samples)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial state is obtained by providing the following to `initialize_particle_filter`:\n",
    "\n",
    "- The generative function for the generative model (`model`)\n",
    "\n",
    "- The initial arguments to the generative function.\n",
    "\n",
    "- The initial observations, expressed as a map from choice address to values (`init_obs`).\n",
    "\n",
    "- The number of particles.\n",
    "\n",
    "At each step, we resample from the collection of traces (`maybe_resample!`) and then we introduce one additional bearing measurement by calling `particle_filter_step!` on the state. We pass the following arguments to `particle_filter_step!`:\n",
    "\n",
    "- The state (it will be mutated)\n",
    "\n",
    "- The new arguments to the generative function for this step. In our case, this is the number of measurements beyond the first measurement.\n",
    "\n",
    "- The [argdiff](https://probcomp.github.io/Gen/dev/ref/gfi/#Argdiffs-1) value, which provides detailed information about the change to the arguments between the previous step and this step. We will revisit this value later. For now, we indicat ethat we do not know how the `T::Int` argument will change with each step.\n",
    "\n",
    "- The new observations associated with the new step. In our case, this just contains the latest measurement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this particle filter with 5000 particles, and return a sample of 100 particles. This will take 30-60 seconds. We will see one way of speeding up the particle filter in a later section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time pf_traces = particle_filter(5000, zs, 200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To render these traces, we first define a function that overlays many renderings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function overlay(renderer, traces; same_data=true, args...)\n",
    "    renderer(traces[1], show_data=true, args...)\n",
    "    for i=2:length(traces)\n",
    "        renderer(traces[i], show_data=!same_data, args...)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then render the traces from the particle filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "overlay(render, pf_traces, same_data=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that as during the period of denser bearing measurements, the trajectories tend to turn so that the heading is more parallel to the bearing vector. An alternative explanation is that the point maintained a constant heading, but just slowed down significantly. It is interesting to see that the inferences favor the \"turning explanation\" over the \"slowing down explanation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Exercise\n",
    "Run the particle filter with fewer particles and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding rejuvenation moves <a name=\"rejuv\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes useful to add MCMC moves to particles in a particle filter between steps. These MCMC moves are often called \"rejuvenation moves\" [4]. Each rejuvenation moves targets the *current posterior distribution* at the given step. For example, when applying the rejuvenation move after incorporating 3 observations, our rejuvenation moves have as their stationary distribution the conditional distribution on the latent variables, given the first three observations.\n",
    "\n",
    "Next, we write a version of the particle filter that applies two random walk Metropolis-Hastings rejuvenation move to each particle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines a Metropolis-Hastings perturbation move that perturbs the velocity vectors for a block of time steps between `a` and `b` inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen function perturbation_proposal(prev_trace, a::Int, b::Int)\n",
    "    choices = get_choices(prev_trace)\n",
    "    (T,) = get_args(prev_trace)\n",
    "    for t=a:b\n",
    "        vx = @trace(normal(choices[(:vx, t)], 1e-3), (:vx, t))\n",
    "        vy = @trace(normal(choices[(:vy, t)], 1e-3), (:vy, t))\n",
    "    end\n",
    "end\n",
    "\n",
    "function perturbation_move(trace, a::Int, b::Int)\n",
    "    Gen.metropolis_hastings(trace, perturbation_proposal, (a, b))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add this into our particle filtering inference program below. We apply the rejuvenation move to adjust the velocities for the previous 5 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function particle_filter_rejuv(num_particles::Int, zs::Vector{Float64}, num_samples::Int)\n",
    "    init_obs = Gen.choicemap((:z0, zs[1]))    \n",
    "    state = Gen.initialize_particle_filter(model, (0,), init_obs, num_particles)\n",
    "    for t=1:length(zs)-1\n",
    "        \n",
    "        # apply a rejuvenation move to each particle\n",
    "        for i=1:num_particles\n",
    "            state.traces[i], _ = perturbation_move(state.traces[i], max(1, t-5), t-1)\n",
    "        end\n",
    "        \n",
    "        Gen.maybe_resample!(state, ess_threshold=num_particles/2)\n",
    "        obs = Gen.choicemap(((:z, t), zs[t+1]))\n",
    "        Gen.particle_filter_step!(state, (t,), (UnknownChange(),), obs)\n",
    "    end\n",
    "    \n",
    "    # return a sample of unweighted traces from the weighted collection\n",
    "    return Gen.sample_unweighted_traces(state, num_samples)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the particle filter with rejuvenation below. This will take a minute or two. We will see one way of speeding up the particle filter in a later section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time pf_rejuv_traces = particle_filter_rejuv(5000, zs, 200);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We render the traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay(render, pf_rejuv_traces, same_data=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using the unfold combinator to improve performance <a name=\"unfold\"></a>\n",
    "\n",
    "For the particle filtering algorithms above, within an update step it is only necessary to revisit the most recent state (or the most recent 5 states if the rejuvenation moves are used) because the initial states are never updated, and the contribution of these states to the weight computation cancel.\n",
    "\n",
    "However, each update step of the particle filter inference programs above scales *linearly* in the size of the trace because it visits every state when computing the weight update. This is because the built-in modeling DSL by default always performs an end-to-end execution of the generative function body whenever performing a trace update. This allows the built-in modeling DSL to be very flexible and to have a simple implementation, at the cost of performance. There are several ways of improving performance after one has a prototype written in the built-in modeling DSL. One of these is [Generative Function Combinators](https://probcomp.github.io/Gen/dev/ref/combinators/), which make the flow of information through the generative process more explicit to Gen, and enable asymptotically more efficient inference programs.\n",
    "\n",
    "To exploit the opportunity for incremental computation, and improve the scaling behavior of our particle filter inference programs, we will write a new model that replaces the following Julia `for` loop in our model, using a generative function combinator.\n",
    "\n",
    "```julia\n",
    "    # generate successive states and measurements\n",
    "    for t=1:T\n",
    "        \n",
    "        # update the state of the point\n",
    "        vx = @trace(normal(vx, sqrt(velocity_var)), (:vx, t))\n",
    "        vy = @trace(normal(vy, sqrt(velocity_var)), (:vy, t))\n",
    "        x += vx\n",
    "        y += vy\n",
    "        \n",
    "        # bearing measurement\n",
    "        @trace(normal(bearing(x, y), measurement_noise), (:z, t))\n",
    "\n",
    "        # record position\n",
    "        xs[t+1] = x\n",
    "        ys[t+1] = y\n",
    "    end\n",
    "```\n",
    "\n",
    "This `for` loop has a very specific pattern of information flow---there is a sequence of states (represented by `x, y, vx, and vy), and each state is generated from the previous state. This is exactly the pattern that the [Unfold](https://probcomp.github.io/Gen/dev/ref/combinators/#Unfold-combinator-1) generative function combinator is designed to handle.\n",
    "\n",
    "Below, we re-express the Julia for loop over the state sequence using the Unfold combinator. Specifically, we define a generative function (kernel) that takes the prevous state as its second argument, and returns the new state. The Unfold combinator takes the kernel and returns a new generative function (chain) that applies kernel repeatedly. Read the Unfold combinator documentation for details on the behavior of the resulting generative function (chain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "struct State\n",
    "    x::Float64\n",
    "    y::Float64\n",
    "    vx::Float64\n",
    "    vy::Float64\n",
    "end\n",
    "\n",
    "@gen (static) function kernel(t::Int, prev_state::State,\n",
    "                              velocity_var::Float64, measurement_noise::Float64)\n",
    "    vx = @trace(normal(prev_state.vx, sqrt(velocity_var)), :vx)\n",
    "    vy = @trace(normal(prev_state.vy, sqrt(velocity_var)), :vy)\n",
    "    x = prev_state.x + vx\n",
    "    y = prev_state.y + vy\n",
    "    @trace(normal(bearing(x, y), measurement_noise), :z)\n",
    "    next_state = State(x, y, vx, vy)\n",
    "    return next_state\n",
    "end\n",
    "\n",
    "chain = Gen.Unfold(kernel)\n",
    "\n",
    "Gen.load_generated_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can understand the behavior of `chain` by getting a trace of it and printing the random choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = Gen.simulate(chain, (4, State(0., 0., 0., 0.), 0.01, 0.01))\n",
    "println(Gen.get_choices(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a new version of the generative model that invokes `chain` instead of using the Julia `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@gen (static) function unfold_model(T::Int)\n",
    "    \n",
    "    # parameters\n",
    "    measurement_noise = 0.005\n",
    "    velocity_var = 1e-6\n",
    "\n",
    "    # initial conditions\n",
    "    x = @trace(normal(0.01, 0.01), :x0)\n",
    "    y = @trace(normal(0.95, 0.01), :y0)\n",
    "    vx = @trace(normal(0.002, 0.01), :vx0)\n",
    "    vy = @trace(normal(-0.013, 0.01), :vy0)\n",
    "\n",
    "    # initial measurement\n",
    "    z = @trace(normal(bearing(x, y), measurement_noise), :z0)\n",
    "\n",
    "    # record initial state\n",
    "    init_state = State(x, y, vx, vy)\n",
    "    \n",
    "    # run `chain` function under address namespace `:chain`, producing a vector of states\n",
    "    states = @trace(chain(T, init_state, velocity_var, measurement_noise), :chain)\n",
    "    \n",
    "    result = (init_state, states)\n",
    "    return result\n",
    "end;\n",
    "\n",
    "Gen.load_generated_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's generate a trace of this new model program to understand its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trace, _) = Gen.generate(unfold_model, (4,))\n",
    "println(Gen.get_choices(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function unfold_particle_filter(num_particles::Int, zs::Vector{Float64}, num_samples::Int)\n",
    "    init_obs = Gen.choicemap((:z0, zs[1]))    \n",
    "    state = Gen.initialize_particle_filter(unfold_model, (0,), init_obs, num_particles)    \n",
    "    for t=1:length(zs)-1\n",
    "\n",
    "        maybe_resample!(state, ess_threshold=num_particles/2)\n",
    "        obs = Gen.choicemap((:chain => t => :z, zs[t+1]))\n",
    "        Gen.particle_filter_step!(state, (t,), (UnknownChange(),), obs)\n",
    "    end\n",
    "    \n",
    "    # return a sample of traces from the weighted collection:\n",
    "    return Gen.sample_unweighted_traces(state, num_samples)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time unfold_pf_traces = unfold_particle_filter(5000, zs, 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function unfold_render(trace; show_data=true, max_T=get_args(trace)[1])\n",
    "    (T,) = Gen.get_args(trace)\n",
    "    choices = Gen.get_choices(trace)\n",
    "    (init_state, states) = Gen.get_retval(trace)\n",
    "    xs = Vector{Float64}(undef, T+1)\n",
    "    ys = Vector{Float64}(undef, T+1)\n",
    "    zs = Vector{Float64}(undef, T+1)\n",
    "    xs[1] = init_state.x\n",
    "    ys[1] = init_state.y\n",
    "    zs[1] = choices[:z0]\n",
    "    for t=1:T\n",
    "        xs[t+1] = states[t].x\n",
    "        ys[t+1] = states[t].y\n",
    "        zs[t+1] = choices[:chain => t => :z]\n",
    "    end\n",
    "    scatter(xs[1:max_T+1], ys[1:max_T+1], s=5)\n",
    "    if show_data\n",
    "        for z in zs[1:max_T+1]\n",
    "            dx = cos(z) * 0.5\n",
    "            dy = sin(z) * 0.5\n",
    "            plot([0., dx], [0., dy], color=\"red\", alpha=0.3)\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the results are reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay(unfold_render, unfold_pf_traces, same_data=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now empirically investigate the scaling behavior of (1) the inference program that uses the Julia `for` loop,  and (2) the equivalent inference program that uses Unfold. We will use a fake long vector of z data, and we will investigate how the running time depends on the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_zs = rand(1000);\n",
    "\n",
    "function timing_experiment(num_observations_list::Vector{Int}, num_particles::Int, num_samples::Int)\n",
    "    times = Vector{Float64}()\n",
    "    times_unfold = Vector{Float64}()\n",
    "    for num_observations in num_observations_list\n",
    "        println(\"evaluating inference programs for num_observations: $num_observations\")\n",
    "        tstart = time_ns()\n",
    "        traces = particle_filter(num_particles, fake_zs[1:num_observations], num_samples)\n",
    "        push!(times, (time_ns() - tstart) / 1e9)\n",
    "        \n",
    "        tstart = time_ns()\n",
    "        traces = unfold_particle_filter(num_particles, fake_zs[1:num_observations], num_samples)\n",
    "        push!(times_unfold, (time_ns() - tstart) / 1e9)\n",
    "        \n",
    "    end\n",
    "    (times, times_unfold)\n",
    "end;\n",
    "\n",
    "num_observations_list = [1, 3, 10, 30, 50, 100, 150, 200, 500]\n",
    "(times, times_unfold) = timing_experiment(num_observations_list, 100, 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the running time of the inference program without unfold appears to be quadratic in the number of observations, whereas the inference program that uses unfold appears to scale linearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(num_observations_list, times, color=\"blue\")\n",
    "plot(num_observations_list, times_unfold, color=\"red\")\n",
    "xlabel(\"Number of observations\")\n",
    "ylabel(\"Running time (sec.)\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
